version: "3.9"
services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama       # cache models
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/version"]
      interval: 10s
      timeout: 3s
      retries: 10

  backend:
    build:
      context: ..
      dockerfile: infra/backend.Dockerfile
    container_name: rag-backend
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - PERSIST_DIR=/app/vector_store/chroma_fedex
      - GENERATION_MODEL=${GENERATION_MODEL:-llama3}
      - EMBED_MODEL=${EMBED_MODEL:-nomic-embed-text}
    volumes:
      - ../vector_store:/app/vector_store      # share your local DB to container
    ports:
      - "8000:8000"
    depends_on:
      ollama:
        condition: service_healthy

volumes:
  ollama_models:
